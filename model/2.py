import numpy as np
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
from datasets import Dataset
import pandas as pd
import warnings
import os
import json
from torch.utils.data import DataLoader
import torch.optim as optim
from sklearn.metrics import precision_recall_fscore_support

warnings.filterwarnings('ignore')

need_to_train = False


# ==================== 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================

class Config:
    # –ü—É—Ç–∏
    DATA_PATH = "–î–∞—Ç–∞—Å–µ—Ç"
    SAVE_DIR = "./multi_label_model"
    BASE_MODEL = "cointegrated/rubert-tiny2"

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    BATCH_SIZE = 8
    LEARNING_RATE = 3e-5
    EPOCHS = 20
    MAX_LENGTH = 128
    TEST_SIZE = 0.2
    SEED = 34

    # –≠–º–æ—Ü–∏–∏ (28 + neutral)
    EMOTIONS = [
        'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',
        'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',
        'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',
        'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',
        'relief', 'remorse', 'sadness', 'surprise', 'neutral'
    ]


# ==================== 2. –ú–û–î–ï–õ–¨ ====================

class EmotionClassifier(nn.Module):
    """–ú–æ–¥–µ–ª—å –¥–ª—è multi-label –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —ç–º–æ—Ü–∏–π"""

    def __init__(self, model_name, num_labels):
        super().__init__()
        # –ó–∞–≥—Ä—É–∂–∞–µ–º BERT
        self.bert = AutoModel.from_pretrained(model_name)
        hidden_size = self.bert.config.hidden_size

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, num_labels),
            nn.Sigmoid()  # –î–ª—è multi-label
        )

    def forward(self, input_ids, attention_mask, labels=None):
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç BERT
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º [CLS] —Ç–æ–∫–µ–Ω
        pooled_output = outputs.last_hidden_state[:, 0, :]
        pooled_output = self.dropout(pooled_output)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        logits = self.classifier(pooled_output)

        # Loss –µ—Å–ª–∏ –µ—Å—Ç—å labels
        loss = None
        if labels is not None:
            loss_fn = nn.BCELoss()
            loss = loss_fn(logits, labels.float())

        return {'loss': loss, 'logits': logits}


# ==================== 3. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ====================

def load_and_prepare_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ"""
    print("=" * 60)
    print("–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
    print("=" * 60)

    # –ó–∞–≥—Ä—É–∑–∫–∞
    df = pd.read_csv(Config.DATA_PATH, encoding='utf-8')
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(df)}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —ç–º–æ—Ü–∏–∏
    available_emotions = [e for e in Config.EMOTIONS if e in df.columns]
    print(f"–ù–∞–π–¥–µ–Ω–æ —ç–º–æ—Ü–∏–π: {len(available_emotions)}")
    print(f"–≠–º–æ—Ü–∏–∏: {available_emotions[:5]}..." if len(available_emotions) > 5 else available_emotions)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
    df['text'] = df['ru_text']
    df['labels'] = df[available_emotions].values.tolist()

    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    df = df[['text', 'labels']]

    # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∞
    df = df.head(10000)  # 500 –ø—Ä–∏–º–µ—Ä–æ–≤
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –°–æ–∑–¥–∞–µ–º mapping
    id2label = {i: emotion for i, emotion in enumerate(available_emotions)}
    label2id = {emotion: i for i, emotion in enumerate(available_emotions)}

    # –°–æ–∑–¥–∞–µ–º Dataset
    dataset = Dataset.from_pandas(df)
    dataset = dataset.train_test_split(
        test_size=Config.TEST_SIZE,
        seed=Config.SEED
    )

    return dataset, available_emotions, id2label, label2id


# ==================== 4. –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø ====================

def tokenize_data(dataset, tokenizer):
    """–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ"""
    print("\n" + "=" * 60)
    print("–¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø")
    print("=" * 60)

    def tokenize_function(examples):
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–µ–∑ token_type_ids –¥–ª—è rubert
        tokenized = tokenizer(
            examples['text'],
            padding='max_length',
            truncation=True,
            max_length=Config.MAX_LENGTH,
            return_tensors=None,
            return_token_type_ids=False,
        )
        tokenized['labels'] = examples['labels']
        return tokenized

    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é
    tokenized_dataset = dataset.map(tokenize_function, batched=True)

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è PyTorch
    tokenized_dataset.set_format(type='torch',
                                 columns=['input_ids', 'attention_mask', 'labels'])

    print(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    print(f"–ü—Ä–∏–º–µ—Ä input_ids: {tokenized_dataset['train'][0]['input_ids'].shape}")

    return tokenized_dataset


# ==================== 5. –û–ë–£–ß–ï–ù–ò–ï ====================

def train_model(model, train_loader, val_loader, epochs=3):
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å"""
    print("\n" + "=" * 60)
    print("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    model.to(device)
    optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)

    history = {'train_loss': [], 'val_loss': []}

    for epoch in range(epochs):
        print(f"\n–≠–ø–æ—Ö–∞ {epoch + 1}/{epochs}")
        print("-" * 40)

        # ===== –û–ë–£–ß–ï–ù–ò–ï =====
        model.train()
        train_loss = 0
        train_batches = 0

        for batch_idx, batch in enumerate(train_loader):
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device).float()

            # Forward
            optimizer.zero_grad()
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            loss = outputs['loss']

            # Backward
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_batches += 1

            if batch_idx % 5 == 0:
                print(f"  Batch {batch_idx:3d}, Loss: {loss.item():.4f}")

        avg_train_loss = train_loss / train_batches
        history['train_loss'].append(avg_train_loss)
        print(f"–°—Ä–µ–¥–Ω–∏–π train loss: {avg_train_loss:.4f}")

        # ===== –í–ê–õ–ò–î–ê–¶–ò–Ø =====
        model.eval()
        val_loss = 0
        val_batches = 0

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device).float()

                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

                val_loss += outputs['loss'].item()
                val_batches += 1

        avg_val_loss = val_loss / val_batches
        history['val_loss'].append(avg_val_loss)
        print(f"–°—Ä–µ–¥–Ω–∏–π val loss: {avg_val_loss:.4f}")

        # ===== –ú–ï–¢–†–ò–ö–ò =====
        if (epoch + 1) % 2 == 0:
            evaluate_model(model, val_loader, device)

    print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    return history


# ==================== 6. –û–¶–ï–ù–ö–ê ====================

def evaluate_model(model, data_loader, device):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å"""
    model.eval()
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].cpu().numpy()

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            preds = (outputs['logits'].cpu().numpy() > 0.5).astype(int)

            all_labels.append(labels)
            all_preds.append(preds)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–∞—Ç—á–∏
    all_labels = np.vstack(all_labels)
    all_preds = np.vstack(all_preds)

    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average='micro', zero_division=0
    )

    # Exact match
    exact_match = np.mean(np.all(all_preds == all_labels, axis=1))

    print(f"  –ú–µ—Ç—Ä–∏–∫–∏: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}, Exact Match={exact_match:.3f}")

    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'exact_match': exact_match
    }


# ==================== 7. –°–û–•–†–ê–ù–ï–ù–ò–ï ====================

def save_model(model, tokenizer, id2label, label2id, save_dir):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
    print("\n" + "=" * 60)
    print("–°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("=" * 60)

    os.makedirs(save_dir, exist_ok=True)

    # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
    torch.save(model.state_dict(), f"{save_dir}/model_weights.pth")
    print(f"‚úÖ –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_dir}/model_weights.pth")

    # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º BERT —á–∞—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ
    model.bert.save_pretrained(save_dir)

    # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer.save_pretrained(save_dir)

    # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = {
        "num_labels": len(id2label),
        "id2label": id2label,
        "label2id": label2id,
        "model_type": "bert",
        "hidden_size": model.bert.config.hidden_size,
        "problem_type": "multi_label_classification",
        "classifier_architecture": str(model.classifier)
    }

    with open(f"{save_dir}/config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
    print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —ç–º–æ—Ü–∏–π: {len(id2label)}")


# ==================== 8. –ó–ê–ì–†–£–ó–ö–ê ====================

def load_model(save_dir, num_labels=None):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
    print("\n" + "=" * 60)
    print("–ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò")
    print("=" * 60)

    if not os.path.exists(save_dir):
        print(f"‚ùå –ü–∞–ø–∫–∞ {save_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return None, None, None, None

    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config_path = f"{save_dir}/config.json"
        if not os.path.exists(config_path):
            print(f"‚ùå –ö–æ–Ω—Ñ–∏–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
            return None, None, None, None

        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)

        id2label = config.get("id2label", {})
        label2id = config.get("label2id", {})
        saved_num_labels = config.get("num_labels", num_labels)

        if num_labels is None:
            num_labels = saved_num_labels

        print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {num_labels} —ç–º–æ—Ü–∏–π")

        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        tokenizer = AutoTokenizer.from_pretrained(save_dir)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else '[PAD]'

        print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")

        # 3. –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        model = EmotionClassifier(Config.BASE_MODEL, num_labels)

        # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
        weights_path = f"{save_dir}/model_weights.pth"
        if os.path.exists(weights_path):
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model.load_state_dict(torch.load(weights_path, map_location=device))
            model.to(device)
            print(f"‚úÖ –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ {device}")
        else:
            print("‚ö†Ô∏è –í–µ—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ —Å –Ω—É–ª—è")

        return model, tokenizer, id2label, label2id

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None


# ==================== 9. –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø ====================

def predict_emotions(model, tokenizer, texts, id2label, threshold=0.3):
    EMOTIONS = [
        'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',
        'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',
        'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',
        'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',
        'relief', 'remorse', 'sadness', 'surprise', 'neutral'
    ]

    EMOTIONS_RU = [
        '–≤–æ—Å—Ö–∏—â–µ–Ω–∏–µ',  # admiration
        '–≤–µ—Å–µ–ª—å–µ',  # amusement
        '–≥–Ω–µ–≤',  # anger
        '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ',  # annoyance
        '–æ–¥–æ–±—Ä–µ–Ω–∏–µ',  # approval
        '–∑–∞–±–æ—Ç–∞',  # caring
        '–∑–∞–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ',  # confusion
        '–ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ',  # curiosity
        '–∂–µ–ª–∞–Ω–∏–µ',  # desire
        '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ',  # disappointment
        '–Ω–µ–æ–¥–æ–±—Ä–µ–Ω–∏–µ',  # disapproval
        '–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ',  # disgust
        '—Å–º—É—â–µ–Ω–∏–µ',  # embarrassment
        '–≤–æ–∑–±—É–∂–¥–µ–Ω–∏–µ',  # excitement
        '—Å—Ç—Ä–∞—Ö',  # fear
        '–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å',  # gratitude
        '–≥–æ—Ä–µ',  # grief
        '—Ä–∞–¥–æ—Å—Ç—å',  # joy
        '–ª—é–±–æ–≤—å',  # love
        '–Ω–µ—Ä–≤–æ–∑–Ω–æ—Å—Ç—å',  # nervousness
        '–æ–ø—Ç–∏–º–∏–∑–º',  # optimism
        '–≥–æ—Ä–¥–æ—Å—Ç—å',  # pride
        '–æ—Å–æ–∑–Ω–∞–Ω–∏–µ',  # realization
        '–æ–±–ª–µ–≥—á–µ–Ω–∏–µ',  # relief
        '—Ä–∞—Å–∫–∞—è–Ω–∏–µ',  # remorse
        '–ø–µ—á–∞–ª—å',  # sadness
        '—É–¥–∏–≤–ª–µ–Ω–∏–µ',  # surprise
        '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å'  # neutral
    ]


    """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —ç–º–æ—Ü–∏–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
    model.eval()
    device = next(model.parameters()).device

    print(f"\n{'=' * 60}")
    print(f"–ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø (–ø–æ—Ä–æ–≥: {threshold})")
    print(f"id2label: {len(id2label)} —ç–º–æ—Ü–∏–π")
    print('=' * 60)

    results = []

    for text_idx, text in enumerate(texts, 1):
        print(f"\n{'=' * 40}")
        print(f"–¢–µ–∫—Å—Ç #{text_idx}: '{text}'")
        print('=' * 40)

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = tokenizer(
            text,
            padding='max_length',
            truncation=True,
            max_length=Config.MAX_LENGTH,
            return_tensors='pt',
            return_token_type_ids=False,
        )

        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        with torch.no_grad():
            outputs = model(**inputs)
            probabilities = outputs['logits'][0].cpu().numpy()

        # 1. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10 —ç–º–æ—Ü–∏–π
        print("\n–¢–æ–ø-10 –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —ç–º–æ—Ü–∏–π:")
        sorted_indices = np.argsort(probabilities)[::-1][:10]
        for rank, idx in enumerate(sorted_indices, 1):
            prob = probabilities[idx]
            emotion_name = id2label.get(idx, f"emotion_{idx}")
            print(f"  {rank:2d}. [{idx:2d}] {EMOTIONS_RU[idx]:20s}: {prob:.3f}")

        # 2. –ù–∞—Ö–æ–¥–∏–º —ç–º–æ—Ü–∏–∏ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
        emotions = []
        for idx, prob in enumerate(probabilities):
            if prob >= threshold:
                emotion_name = id2label.get(idx, f"emotion_{idx}")
                emotions.append({
                    'emotion': EMOTIONS_RU[idx],
                    'probability': float(prob),
                    'idx': idx
                })

        # 3. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        emotions.sort(key=lambda x: x['probability'], reverse=True)

        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        results.append({
            'text': text,
            'emotions': emotions,
            'probabilities': probabilities.tolist(),
            'max_probability': float(np.max(probabilities))
        })

        # 5. –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if emotions:
            print(f"\nüéØ –≠–ú–û–¶–ò–ò –í–´–®–ï –ü–û–†–û–ì–ê ({threshold}):")
            for emotion in emotions[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                print(f"  ‚Ä¢ {emotion['emotion']}: {emotion['probability']:.3f}")
            if len(emotions) > 5:
                print(f"    ... –∏ –µ—â–µ {len(emotions) - 5} —ç–º–æ—Ü–∏–π")
        else:
            max_idx = np.argmax(probabilities)
            max_prob = probabilities[max_idx]
            max_emotion = id2label.get(max_idx, f"emotion_{max_idx}")
            print(f"\n‚ö†Ô∏è –ù–µ—Ç —ç–º–æ—Ü–∏–π –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ {threshold}")
            print(f"   –ù–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–∞—è: {max_emotion} ({max_prob:.3f})")

    return results


# ==================== 10. –û–°–ù–û–í–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê ====================

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 60)
    print("–ú–û–î–ï–õ–¨ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –≠–ú–û–¶–ò–ô")
    print("=" * 60)

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    dataset, available_emotions, id2label, label2id = load_and_prepare_data()

    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model, tokenizer, loaded_id2label, loaded_label2id = load_model(Config.SAVE_DIR,
                                                                    len(available_emotions))

    if model is None:
        print("\nüÜï –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        tokenizer = AutoTokenizer.from_pretrained(Config.BASE_MODEL)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else '[PAD]'

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        num_labels = len(available_emotions)
        model = EmotionClassifier(Config.BASE_MODEL, num_labels)

        print(f"‚úÖ –ù–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {num_labels} —ç–º–æ—Ü–∏–π")
    else:
        print("\n‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ mapping
        if loaded_id2label:
            id2label = loaded_id2label
            label2id = loaded_label2id

    # 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    tokenized_dataset = tokenize_data(dataset, tokenizer)

    # 4. –°–æ–∑–¥–∞–µ–º DataLoader
    train_loader = DataLoader(
        tokenized_dataset['train'],
        batch_size=Config.BATCH_SIZE,
        shuffle=True
    )
    val_loader = DataLoader(
        tokenized_dataset['test'],
        batch_size=Config.BATCH_SIZE
    )

    print(f"\n–†–∞–∑–º–µ—Ä train loader: {len(train_loader)} –±–∞—Ç—á–µ–π")
    print(f"–†–∞–∑–º–µ—Ä val loader: {len(val_loader)} –±–∞—Ç—á–µ–π")

    if need_to_train:
        # 5. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        history = train_model(model, train_loader, val_loader, epochs=Config.EPOCHS)

        # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        save_model(model, tokenizer, id2label, label2id, Config.SAVE_DIR)

    # 7. –¢–µ—Å—Ç–∏—Ä—É–µ–º
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï")
    print("=" * 60)

    # test_texts = [
    #     "–Ø –æ—á–µ–Ω—å —Ä–∞–¥ —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏!",
    #     "–ú–Ω–µ —Å—Ç—Ä–∞—à–Ω–æ –∏ —Ç—Ä–µ–≤–æ–∂–Ω–æ",
    #     "–≠—Ç–æ –∑–ª–∏—Ç –∏ —Ä–∞–∑–¥—Ä–∞–∂–∞–µ—Ç –º–µ–Ω—è",
    #     "–ß—É–≤—Å—Ç–≤—É—é –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å –∏ –ª—é–±–æ–≤—å",
    #     "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"
    # ]

    test_texts = ["–Ø –ø–æ–ª—É—á–∏–ª 2 –ø–æ —ç–∫–∑–∞–º–µ–Ω—É",
                  "–Ø –ø–æ–ª—É—á–∏–ª 5 –ø–æ —ç–∫–∑–∞–º–µ–Ω—É"]

    predictions = predict_emotions(model, tokenizer, test_texts, id2label, threshold=0.4)

    for pred in predictions:
        print(f"\n–¢–µ–∫—Å—Ç: '{pred['text']}'")
        print(f"–ú–∞–∫—Å. –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {pred['max_probability']:.3f}")

        if pred['emotions']:
            print("–≠–º–æ—Ü–∏–∏:")
            for emotion in pred['emotions']:
                print(f"  ‚Ä¢ {emotion['emotion']}: {emotion['probability']:.3f}")
        else:
            print("–ù–µ—Ç —ç–º–æ—Ü–∏–π –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞")

    # 8. –í—ã–≤–æ–¥ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    # print("\n" + "=" * 60)
    # print("–ò–°–¢–û–†–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø")
    # print("=" * 60)
    #
    # for epoch, (train_loss, val_loss) in enumerate(zip(history['train_loss'], history['val_loss'])):
    #     print(f"–≠–ø–æ—Ö–∞ {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}")

    print("\n" + "=" * 60)
    print("–ü–†–û–ì–†–ê–ú–ú–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("=" * 60)


# ==================== –ó–ê–ü–£–°–ö ====================

if __name__ == "__main__":
    main()